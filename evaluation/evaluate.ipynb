{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_path (model):\n",
    "    filePath=f\"/Users/sden118/Desktop/FinReasoning/test/{model}_test.json\"\n",
    "    return filePath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "    a=data[\"isCorrect\"].sum()\n",
    "    b=data['isCorrect'].count()\n",
    "    accuracy=(a/b*100).__round__(2)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    # data=data[data['Question Type']==\"text+image\"]\n",
    "    \n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "    a=data[\"isCorrect\"].sum()\n",
    "    b=data['isCorrect'].count()\n",
    "    overall=(a/b*100).__round__(2)\n",
    "\n",
    "    df1=data[data[\"QA Type\"]==\"Knowledge Reasoning QA\"]\n",
    "   \n",
    "    c=df1[\"isCorrect\"].sum()\n",
    "    d=df1['isCorrect'].count()\n",
    "    expertise=(c/d*100).__round__(2)\n",
    "\n",
    "    df2=data[data[\"QA Type\"]==\"Math Reasoning QA\"]\n",
    "    \n",
    "    e=df2[\"isCorrect\"].sum()\n",
    "    f=df2['isCorrect'].count()\n",
    "    math=(e/f*100).__round__(2)\n",
    "    return overall,expertise,math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topics(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    # 计算每个答案是否正确\n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "\n",
    "    data['General Topics']=data['General Topics'].apply(lambda x: 'Others' if x in other_topics  else x)\n",
    "\n",
    "    # 计算每个 General Topics 的正确答案数量\n",
    "    correct_by_topic = data.groupby('General Topics')['isCorrect'].sum().reset_index()\n",
    "\n",
    "    # 计算每个 General Topics 的总答案数量\n",
    "    count_by_topic = data.groupby('General Topics')['isCorrect'].count().reset_index()\n",
    "\n",
    "    # 合并两个 DataFrame\n",
    "    accuracy_by_topic = pd.merge(correct_by_topic, count_by_topic, on='General Topics')\n",
    "\n",
    "    # 计算正确率\n",
    "    accuracy_by_topic['accuracy%'] = round(accuracy_by_topic['isCorrect_x'] / accuracy_by_topic['isCorrect_y'] * 100, 2)\n",
    "\n",
    "    # 重命名列\n",
    "    accuracy_by_topic.columns = ['General Topics', 'correct', 'total', 'accuracy%']\n",
    "\n",
    "    # 计算总体正确率\n",
    "    total_correct = data['isCorrect'].sum()\n",
    "    total_count = data['isCorrect'].count()\n",
    "    overall_accuracy = round((total_correct / total_count) * 100, 2)\n",
    "\n",
    "    # 创建一个新的 DataFrame 来存储总体正确率\n",
    "    overall_accuracy_df = pd.DataFrame([['Overall', total_correct, total_count, overall_accuracy]], columns=['General Topics', 'correct', 'total', 'accuracy%'])\n",
    "\n",
    "    # 将总体正确率 DataFrame 与 accuracy_by_topic 进行合并\n",
    "    final_df = pd.concat([accuracy_by_topic, overall_accuracy_df], ignore_index=True)\n",
    "\n",
    "    # 打印结果\n",
    "    accuracy=list(final_df['accuracy%'])\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topics_test(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    data=data[data['Question Type']==\"text+image\"]\n",
    "    # 计算每个答案是否正确\n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "\n",
    "    data['General Topics']=data['General Topics'].apply(lambda x: 'Others' if x in other_topics  else x)\n",
    "\n",
    "    # 计算每个 General Topics 的正确答案数量\n",
    "    correct_by_topic = data.groupby('General Topics')['isCorrect'].sum().reset_index()\n",
    "\n",
    "    # 计算每个 General Topics 的总答案数量\n",
    "    count_by_topic = data.groupby('General Topics')['isCorrect'].count().reset_index()\n",
    "\n",
    "    # 合并两个 DataFrame\n",
    "    accuracy_by_topic = pd.merge(correct_by_topic, count_by_topic, on='General Topics')\n",
    "\n",
    "    # 计算正确率\n",
    "    accuracy_by_topic['accuracy%'] = round(accuracy_by_topic['isCorrect_x'] / accuracy_by_topic['isCorrect_y'] * 100, 2)\n",
    "\n",
    "    # 重命名列\n",
    "    accuracy_by_topic.columns = ['General Topics', 'correct', 'total', 'accuracy%']\n",
    "\n",
    "    # 计算总体正确率\n",
    "    total_correct = data['isCorrect'].sum()\n",
    "    total_count = data['isCorrect'].count()\n",
    "    overall_accuracy = round((total_correct / total_count) * 100, 2)\n",
    "\n",
    "    # 创建一个新的 DataFrame 来存储总体正确率\n",
    "    overall_accuracy_df = pd.DataFrame([['Overall', total_correct, total_count, overall_accuracy]], columns=['General Topics', 'correct', 'total', 'accuracy%'])\n",
    "\n",
    "    # 将总体正确率 DataFrame 与 accuracy_by_topic 进行合并\n",
    "    final_df = pd.concat([accuracy_by_topic, overall_accuracy_df], ignore_index=True)\n",
    "\n",
    "    # 打印结果\n",
    "    accuracy=list(final_df['accuracy%'])\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[\n",
    "        \"gpt_o1\",\"gpt_o1_rag\",\n",
    "        \"Gemini_Pro1\",\"Gemini_Pro1_rag\",\n",
    "        \"deepseek\",\"deepseek_rag\",\n",
    "        \"qwen\",\"qwen_rag\",\n",
    "        \"llama\",\"llama_rag\",\n",
    "        \"claude_sonnet\",\"claude_sonnet_rag\",\n",
    "        \n",
    "        \"gpt_4o\",\"gpt_4o_rag\",\n",
    "        \"gemini_1.5_pro\",\"gemini_1.5_pro_rag\",\n",
    "        \"claude_3.5\",\"claude_3.5_rag\",\n",
    "        \"qwen2_vl\",\"qwen2_vl_rag\",\n",
    "        \"Llava\",\"Llava_rag\",\n",
    "        \"Llama_3.2_vision\",\"Llama_3.2_vision_rag\",\n",
    "        \n",
    "        ]\n",
    "topics=['Alternative Investments',\n",
    "#  'Asset Allocation',\n",
    "#  'Capital Market Expectations',\n",
    " 'Corporate Finance',\n",
    " 'Credit Risk Measurement and Management',\n",
    " 'Derivatives',\n",
    "#  'Derivatives and Currency Management',\n",
    " 'Economics',\n",
    " 'Equity Investments',\n",
    "#  'Equity Portfolio Management',\n",
    "#  'Ethicaland Professional Standards',\n",
    " 'Financial Markets and Products',\n",
    " 'Financial Reporting and Analysis',\n",
    " 'Fixed Income',\n",
    "#  'Fixed-Income Portfolio Management',\n",
    " 'Foundation of Risk Management',\n",
    " 'Liquidity and Treasury Risk Measurement',\n",
    " 'Market Risk Measurement and Management',\n",
    " 'Operational Risk',\n",
    "#  'Operational Risk and Resiliency',\n",
    " 'Portfolio Management',\n",
    " 'Quantitative Methods',\n",
    " 'Risk Management and Investment Management',\n",
    " 'Valuation and Risk Models',\n",
    " 'Overall']\n",
    "other_topics=['Fixed-Income Portfolio Management','Operational Risk','Equity Portfolio Management','Derivatives and Currency Management',\n",
    "              'Asset Allocation','Ethicaland Professional Standards','Capital Market Expectations']\n",
    "column=['Alternative Investments',\n",
    "#  'Asset Allocation',\n",
    "#  'Capital Market Expectations',\n",
    " 'Corporate Finance',\n",
    " 'Credit Risk Measurement and Management',\n",
    " 'Derivatives',\n",
    "#  'Derivatives and Currency Management',\n",
    " 'Economics',\n",
    " 'Equity Investments',\n",
    "#  'Equity Portfolio Management',\n",
    "#  'Ethicaland Professional Standards',\n",
    " 'Financial Markets and Products',\n",
    " 'Financial Reporting and Analysis',\n",
    " 'Fixed Income',\n",
    "#  'Fixed-Income Portfolio Management',\n",
    " 'Foundation of Risk Management',\n",
    " 'Liquidity and Treasury Risk Measurement',\n",
    " 'Market Risk Measurement and Management',\n",
    " 'Operational Risk',\n",
    "#  'Operational Risk and Resiliency',\n",
    " 'Portfolio Management',\n",
    " 'Quantitative Methods',\n",
    " 'Risk Management and Investment Management',\n",
    " 'Valuation and Risk Models',\n",
    " \"Others\",\n",
    " 'Overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Overall%</th>\n",
       "      <th>Expertise%</th>\n",
       "      <th>Math%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_o1</td>\n",
       "      <td>46.56</td>\n",
       "      <td>55.65</td>\n",
       "      <td>33.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_o1_rag</td>\n",
       "      <td>60.31</td>\n",
       "      <td>68.28</td>\n",
       "      <td>49.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini_Pro1</td>\n",
       "      <td>47.81</td>\n",
       "      <td>57.26</td>\n",
       "      <td>34.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemini_Pro1_rag</td>\n",
       "      <td>61.37</td>\n",
       "      <td>68.45</td>\n",
       "      <td>51.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>61.25</td>\n",
       "      <td>57.53</td>\n",
       "      <td>66.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek_rag</td>\n",
       "      <td>71.88</td>\n",
       "      <td>66.67</td>\n",
       "      <td>79.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>qwen</td>\n",
       "      <td>55.62</td>\n",
       "      <td>57.53</td>\n",
       "      <td>52.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>qwen_rag</td>\n",
       "      <td>67.97</td>\n",
       "      <td>67.74</td>\n",
       "      <td>68.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama</td>\n",
       "      <td>27.34</td>\n",
       "      <td>30.11</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama_rag</td>\n",
       "      <td>36.09</td>\n",
       "      <td>38.98</td>\n",
       "      <td>32.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>claude_sonnet</td>\n",
       "      <td>53.91</td>\n",
       "      <td>61.83</td>\n",
       "      <td>42.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>claude_sonnet_rag</td>\n",
       "      <td>64.84</td>\n",
       "      <td>71.24</td>\n",
       "      <td>55.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>72.19</td>\n",
       "      <td>73.17</td>\n",
       "      <td>71.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt_4o_rag</td>\n",
       "      <td>81.72</td>\n",
       "      <td>81.03</td>\n",
       "      <td>83.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>70.83</td>\n",
       "      <td>71.24</td>\n",
       "      <td>70.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemini_1.5_pro_rag</td>\n",
       "      <td>82.06</td>\n",
       "      <td>81.18</td>\n",
       "      <td>83.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>claude_3.5</td>\n",
       "      <td>75.94</td>\n",
       "      <td>77.96</td>\n",
       "      <td>73.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>claude_3.5_rag</td>\n",
       "      <td>80.78</td>\n",
       "      <td>81.18</td>\n",
       "      <td>80.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qwen2_vl</td>\n",
       "      <td>52.66</td>\n",
       "      <td>59.14</td>\n",
       "      <td>43.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen2_vl_rag</td>\n",
       "      <td>65.00</td>\n",
       "      <td>70.70</td>\n",
       "      <td>57.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Llava</td>\n",
       "      <td>16.72</td>\n",
       "      <td>18.55</td>\n",
       "      <td>14.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Llava_rag</td>\n",
       "      <td>28.28</td>\n",
       "      <td>29.30</td>\n",
       "      <td>26.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Llama_3.2_vision</td>\n",
       "      <td>3.21</td>\n",
       "      <td>4.19</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Llama_3.2_vision_rag</td>\n",
       "      <td>9.04</td>\n",
       "      <td>9.42</td>\n",
       "      <td>8.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Overall%  Expertise%  Math%\n",
       "0                 gpt_o1     46.56       55.65  33.96\n",
       "1             gpt_o1_rag     60.31       68.28  49.25\n",
       "2            Gemini_Pro1     47.81       57.26  34.70\n",
       "3        Gemini_Pro1_rag     61.37       68.45  51.49\n",
       "4               deepseek     61.25       57.53  66.42\n",
       "5           deepseek_rag     71.88       66.67  79.10\n",
       "6                   qwen     55.62       57.53  52.99\n",
       "7               qwen_rag     67.97       67.74  68.28\n",
       "8                  llama     27.34       30.11  23.51\n",
       "9              llama_rag     36.09       38.98  32.09\n",
       "10         claude_sonnet     53.91       61.83  42.91\n",
       "11     claude_sonnet_rag     64.84       71.24  55.97\n",
       "12                gpt_4o     72.19       73.17  71.43\n",
       "13            gpt_4o_rag     81.72       81.03  83.08\n",
       "14        gemini_1.5_pro     70.83       71.24  70.26\n",
       "15    gemini_1.5_pro_rag     82.06       81.18  83.27\n",
       "16            claude_3.5     75.94       77.96  73.13\n",
       "17        claude_3.5_rag     80.78       81.18  80.22\n",
       "18              qwen2_vl     52.66       59.14  43.66\n",
       "19          qwen2_vl_rag     65.00       70.70  57.09\n",
       "20                 Llava     16.72       18.55  14.18\n",
       "21             Llava_rag     28.28       29.30  26.87\n",
       "22      Llama_3.2_vision      3.21        4.19   1.97\n",
       "23  Llama_3.2_vision_rag      9.04        9.42   8.55"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAcc=[]\n",
    "expertiseAcc=[]\n",
    "mathAcc=[]\n",
    "for model in models:\n",
    "    a,b,c=evaluate_test(file_path(model))\n",
    "    modelAcc.append(a)\n",
    "    expertiseAcc.append(b)\n",
    "    mathAcc.append(c)\n",
    "result={\n",
    "    'Model': models,\n",
    "    'Overall%': modelAcc,\n",
    "    'Expertise%': expertiseAcc,\n",
    "    'Math%': mathAcc\n",
    "\n",
    "}\n",
    "\n",
    "result_test=pd.DataFrame(result)\n",
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test.to_csv('/Users/sden118/Desktop/FinReasoning/evaluation/Model_accuracy_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_o1</td>\n",
       "      <td>67.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_o1_rag</td>\n",
       "      <td>80.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini_Pro1</td>\n",
       "      <td>70.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemini_Pro1_rag</td>\n",
       "      <td>78.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek_rag</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama_rag</td>\n",
       "      <td>47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>79.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt_4o_rag</td>\n",
       "      <td>85.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemini_1.5_pro_rag</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qwen</td>\n",
       "      <td>66.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qwen_rag</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Llava</td>\n",
       "      <td>28.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Llava_rag</td>\n",
       "      <td>35.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Llama_3.2_vision</td>\n",
       "      <td>22.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama_3.2_vision_rag</td>\n",
       "      <td>36.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qwen2_vl</td>\n",
       "      <td>73.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen2_vl_rag</td>\n",
       "      <td>80.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy%\n",
       "0                 gpt_o1      67.67\n",
       "1             gpt_o1_rag      80.41\n",
       "2            Gemini_Pro1      70.39\n",
       "3        Gemini_Pro1_rag      78.68\n",
       "4               deepseek      70.90\n",
       "5           deepseek_rag      78.75\n",
       "6                  llama      35.20\n",
       "7              llama_rag      47.95\n",
       "8                 gpt_4o      79.69\n",
       "9             gpt_4o_rag      85.28\n",
       "10        gemini_1.5_pro      77.53\n",
       "11    gemini_1.5_pro_rag      86.00\n",
       "12                  qwen      66.02\n",
       "13              qwen_rag      76.68\n",
       "14                 Llava      28.67\n",
       "15             Llava_rag      35.47\n",
       "16      Llama_3.2_vision      22.06\n",
       "17  Llama_3.2_vision_rag      36.88\n",
       "18              qwen2_vl      73.27\n",
       "19          qwen2_vl_rag      80.40"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAcc=[]\n",
    "for model in models:\n",
    "    modelAcc.append(evaluate(file_path(model)))\n",
    "result={\n",
    "    'Model': models,\n",
    "    'Accuracy%': modelAcc\n",
    "}\n",
    "\n",
    "result=pd.DataFrame(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/Volumes/Jennie/Reasoning/FinReasoning/evaluation/Model_Accuracy.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemini_1.5_pro_rag</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt_4o_rag</td>\n",
       "      <td>85.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_o1_rag</td>\n",
       "      <td>80.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>79.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek_rag</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemini_Pro1_rag</td>\n",
       "      <td>78.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qwen_rag</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini_Pro1</td>\n",
       "      <td>70.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_o1</td>\n",
       "      <td>67.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qwen</td>\n",
       "      <td>66.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama_rag</td>\n",
       "      <td>47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama_3.2_vision_rag</td>\n",
       "      <td>36.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Llava_rag</td>\n",
       "      <td>35.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Llava</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Llama_3.2_vision</td>\n",
       "      <td>22.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy%\n",
       "11    gemini_1.5_pro_rag      86.00\n",
       "9             gpt_4o_rag      85.28\n",
       "1             gpt_o1_rag      80.41\n",
       "8                 gpt_4o      79.69\n",
       "5           deepseek_rag      78.75\n",
       "3        Gemini_Pro1_rag      78.68\n",
       "10        gemini_1.5_pro      77.53\n",
       "13              qwen_rag      76.68\n",
       "4               deepseek      70.90\n",
       "2            Gemini_Pro1      70.39\n",
       "0                 gpt_o1      67.67\n",
       "12                  qwen      66.02\n",
       "7              llama_rag      47.95\n",
       "17  Llama_3.2_vision_rag      36.88\n",
       "15             Llava_rag      35.47\n",
       "6                  llama      35.20\n",
       "14                 Llava      28.73\n",
       "16      Llama_3.2_vision      22.06"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='Accuracy%', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alternative Investments</th>\n",
       "      <th>Corporate Finance</th>\n",
       "      <th>Credit Risk Measurement and Management</th>\n",
       "      <th>Derivatives</th>\n",
       "      <th>Economics</th>\n",
       "      <th>Equity Investments</th>\n",
       "      <th>Financial Markets and Products</th>\n",
       "      <th>Financial Reporting and Analysis</th>\n",
       "      <th>Fixed Income</th>\n",
       "      <th>Foundation of Risk Management</th>\n",
       "      <th>Liquidity and Treasury Risk Measurement</th>\n",
       "      <th>Market Risk Measurement and Management</th>\n",
       "      <th>Operational Risk</th>\n",
       "      <th>Portfolio Management</th>\n",
       "      <th>Quantitative Methods</th>\n",
       "      <th>Risk Management and Investment Management</th>\n",
       "      <th>Valuation and Risk Models</th>\n",
       "      <th>Others</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt_o1</th>\n",
       "      <td>67.50</td>\n",
       "      <td>67.71</td>\n",
       "      <td>71.07</td>\n",
       "      <td>65.98</td>\n",
       "      <td>74.68</td>\n",
       "      <td>63.58</td>\n",
       "      <td>74.16</td>\n",
       "      <td>67.07</td>\n",
       "      <td>78.49</td>\n",
       "      <td>63.23</td>\n",
       "      <td>61.76</td>\n",
       "      <td>63.72</td>\n",
       "      <td>57.14</td>\n",
       "      <td>51.10</td>\n",
       "      <td>65.79</td>\n",
       "      <td>77.26</td>\n",
       "      <td>55.66</td>\n",
       "      <td>67.13</td>\n",
       "      <td>67.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_o1_rag</th>\n",
       "      <td>77.50</td>\n",
       "      <td>83.33</td>\n",
       "      <td>81.76</td>\n",
       "      <td>79.38</td>\n",
       "      <td>85.99</td>\n",
       "      <td>77.78</td>\n",
       "      <td>87.64</td>\n",
       "      <td>77.33</td>\n",
       "      <td>88.71</td>\n",
       "      <td>75.97</td>\n",
       "      <td>76.24</td>\n",
       "      <td>74.34</td>\n",
       "      <td>70.33</td>\n",
       "      <td>65.38</td>\n",
       "      <td>80.42</td>\n",
       "      <td>86.92</td>\n",
       "      <td>70.75</td>\n",
       "      <td>84.08</td>\n",
       "      <td>80.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1</th>\n",
       "      <td>76.25</td>\n",
       "      <td>65.62</td>\n",
       "      <td>49.37</td>\n",
       "      <td>80.41</td>\n",
       "      <td>81.65</td>\n",
       "      <td>76.54</td>\n",
       "      <td>76.32</td>\n",
       "      <td>71.43</td>\n",
       "      <td>59.12</td>\n",
       "      <td>66.45</td>\n",
       "      <td>67.65</td>\n",
       "      <td>64.60</td>\n",
       "      <td>71.43</td>\n",
       "      <td>58.01</td>\n",
       "      <td>76.84</td>\n",
       "      <td>81.62</td>\n",
       "      <td>60.38</td>\n",
       "      <td>68.51</td>\n",
       "      <td>70.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1_rag</th>\n",
       "      <td>81.25</td>\n",
       "      <td>78.12</td>\n",
       "      <td>62.42</td>\n",
       "      <td>83.51</td>\n",
       "      <td>84.81</td>\n",
       "      <td>84.57</td>\n",
       "      <td>84.91</td>\n",
       "      <td>78.78</td>\n",
       "      <td>69.36</td>\n",
       "      <td>72.90</td>\n",
       "      <td>80.39</td>\n",
       "      <td>72.57</td>\n",
       "      <td>81.32</td>\n",
       "      <td>70.98</td>\n",
       "      <td>83.68</td>\n",
       "      <td>85.67</td>\n",
       "      <td>70.75</td>\n",
       "      <td>79.58</td>\n",
       "      <td>78.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek</th>\n",
       "      <td>73.75</td>\n",
       "      <td>69.79</td>\n",
       "      <td>58.49</td>\n",
       "      <td>73.20</td>\n",
       "      <td>80.38</td>\n",
       "      <td>76.54</td>\n",
       "      <td>72.66</td>\n",
       "      <td>73.17</td>\n",
       "      <td>77.96</td>\n",
       "      <td>69.68</td>\n",
       "      <td>53.92</td>\n",
       "      <td>55.75</td>\n",
       "      <td>64.84</td>\n",
       "      <td>54.40</td>\n",
       "      <td>78.95</td>\n",
       "      <td>82.87</td>\n",
       "      <td>63.21</td>\n",
       "      <td>69.20</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_rag</th>\n",
       "      <td>80.00</td>\n",
       "      <td>80.21</td>\n",
       "      <td>65.41</td>\n",
       "      <td>82.47</td>\n",
       "      <td>88.61</td>\n",
       "      <td>82.10</td>\n",
       "      <td>81.65</td>\n",
       "      <td>80.08</td>\n",
       "      <td>83.33</td>\n",
       "      <td>74.19</td>\n",
       "      <td>70.59</td>\n",
       "      <td>63.72</td>\n",
       "      <td>71.43</td>\n",
       "      <td>63.89</td>\n",
       "      <td>85.79</td>\n",
       "      <td>88.16</td>\n",
       "      <td>73.58</td>\n",
       "      <td>79.58</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama</th>\n",
       "      <td>36.25</td>\n",
       "      <td>37.50</td>\n",
       "      <td>49.69</td>\n",
       "      <td>26.80</td>\n",
       "      <td>43.04</td>\n",
       "      <td>38.89</td>\n",
       "      <td>25.09</td>\n",
       "      <td>35.37</td>\n",
       "      <td>67.20</td>\n",
       "      <td>32.90</td>\n",
       "      <td>24.51</td>\n",
       "      <td>19.47</td>\n",
       "      <td>37.36</td>\n",
       "      <td>26.92</td>\n",
       "      <td>31.58</td>\n",
       "      <td>40.19</td>\n",
       "      <td>26.42</td>\n",
       "      <td>26.99</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_rag</th>\n",
       "      <td>44.87</td>\n",
       "      <td>46.88</td>\n",
       "      <td>59.87</td>\n",
       "      <td>41.24</td>\n",
       "      <td>58.86</td>\n",
       "      <td>49.38</td>\n",
       "      <td>40.47</td>\n",
       "      <td>46.34</td>\n",
       "      <td>77.96</td>\n",
       "      <td>48.37</td>\n",
       "      <td>42.00</td>\n",
       "      <td>35.40</td>\n",
       "      <td>50.55</td>\n",
       "      <td>35.71</td>\n",
       "      <td>47.37</td>\n",
       "      <td>52.34</td>\n",
       "      <td>40.00</td>\n",
       "      <td>38.49</td>\n",
       "      <td>47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o</th>\n",
       "      <td>76.25</td>\n",
       "      <td>75.79</td>\n",
       "      <td>72.90</td>\n",
       "      <td>85.57</td>\n",
       "      <td>89.03</td>\n",
       "      <td>86.42</td>\n",
       "      <td>81.58</td>\n",
       "      <td>85.31</td>\n",
       "      <td>88.52</td>\n",
       "      <td>84.40</td>\n",
       "      <td>74.49</td>\n",
       "      <td>63.39</td>\n",
       "      <td>69.66</td>\n",
       "      <td>63.22</td>\n",
       "      <td>85.26</td>\n",
       "      <td>85.05</td>\n",
       "      <td>70.87</td>\n",
       "      <td>75.00</td>\n",
       "      <td>79.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o_rag</th>\n",
       "      <td>88.75</td>\n",
       "      <td>87.37</td>\n",
       "      <td>81.29</td>\n",
       "      <td>89.69</td>\n",
       "      <td>90.32</td>\n",
       "      <td>88.89</td>\n",
       "      <td>87.97</td>\n",
       "      <td>89.80</td>\n",
       "      <td>91.80</td>\n",
       "      <td>88.73</td>\n",
       "      <td>82.65</td>\n",
       "      <td>71.43</td>\n",
       "      <td>73.03</td>\n",
       "      <td>74.14</td>\n",
       "      <td>87.89</td>\n",
       "      <td>87.54</td>\n",
       "      <td>80.58</td>\n",
       "      <td>81.60</td>\n",
       "      <td>85.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro</th>\n",
       "      <td>81.25</td>\n",
       "      <td>83.33</td>\n",
       "      <td>66.67</td>\n",
       "      <td>80.41</td>\n",
       "      <td>86.71</td>\n",
       "      <td>83.33</td>\n",
       "      <td>75.66</td>\n",
       "      <td>79.67</td>\n",
       "      <td>82.26</td>\n",
       "      <td>84.52</td>\n",
       "      <td>69.61</td>\n",
       "      <td>65.49</td>\n",
       "      <td>71.43</td>\n",
       "      <td>59.34</td>\n",
       "      <td>82.63</td>\n",
       "      <td>89.10</td>\n",
       "      <td>73.58</td>\n",
       "      <td>70.59</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro_rag</th>\n",
       "      <td>90.00</td>\n",
       "      <td>90.62</td>\n",
       "      <td>74.84</td>\n",
       "      <td>89.69</td>\n",
       "      <td>92.41</td>\n",
       "      <td>92.59</td>\n",
       "      <td>84.27</td>\n",
       "      <td>86.59</td>\n",
       "      <td>89.78</td>\n",
       "      <td>88.39</td>\n",
       "      <td>81.37</td>\n",
       "      <td>76.99</td>\n",
       "      <td>76.92</td>\n",
       "      <td>76.37</td>\n",
       "      <td>87.37</td>\n",
       "      <td>92.83</td>\n",
       "      <td>80.19</td>\n",
       "      <td>86.16</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen</th>\n",
       "      <td>71.25</td>\n",
       "      <td>56.25</td>\n",
       "      <td>53.46</td>\n",
       "      <td>62.89</td>\n",
       "      <td>77.85</td>\n",
       "      <td>75.31</td>\n",
       "      <td>64.04</td>\n",
       "      <td>67.48</td>\n",
       "      <td>69.89</td>\n",
       "      <td>68.39</td>\n",
       "      <td>52.94</td>\n",
       "      <td>40.71</td>\n",
       "      <td>67.03</td>\n",
       "      <td>50.00</td>\n",
       "      <td>72.11</td>\n",
       "      <td>82.35</td>\n",
       "      <td>65.09</td>\n",
       "      <td>63.32</td>\n",
       "      <td>66.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_rag</th>\n",
       "      <td>85.00</td>\n",
       "      <td>71.88</td>\n",
       "      <td>67.30</td>\n",
       "      <td>74.23</td>\n",
       "      <td>84.81</td>\n",
       "      <td>83.33</td>\n",
       "      <td>74.53</td>\n",
       "      <td>78.05</td>\n",
       "      <td>79.03</td>\n",
       "      <td>78.06</td>\n",
       "      <td>67.65</td>\n",
       "      <td>61.95</td>\n",
       "      <td>78.02</td>\n",
       "      <td>60.44</td>\n",
       "      <td>78.95</td>\n",
       "      <td>88.85</td>\n",
       "      <td>75.47</td>\n",
       "      <td>76.47</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava</th>\n",
       "      <td>26.25</td>\n",
       "      <td>25.00</td>\n",
       "      <td>22.64</td>\n",
       "      <td>35.05</td>\n",
       "      <td>35.44</td>\n",
       "      <td>34.57</td>\n",
       "      <td>26.59</td>\n",
       "      <td>36.59</td>\n",
       "      <td>38.71</td>\n",
       "      <td>24.52</td>\n",
       "      <td>17.65</td>\n",
       "      <td>23.01</td>\n",
       "      <td>35.16</td>\n",
       "      <td>17.03</td>\n",
       "      <td>34.74</td>\n",
       "      <td>28.97</td>\n",
       "      <td>25.47</td>\n",
       "      <td>24.57</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava_rag</th>\n",
       "      <td>38.75</td>\n",
       "      <td>35.42</td>\n",
       "      <td>28.30</td>\n",
       "      <td>45.36</td>\n",
       "      <td>39.87</td>\n",
       "      <td>43.21</td>\n",
       "      <td>31.84</td>\n",
       "      <td>43.90</td>\n",
       "      <td>47.31</td>\n",
       "      <td>30.32</td>\n",
       "      <td>25.49</td>\n",
       "      <td>30.97</td>\n",
       "      <td>39.56</td>\n",
       "      <td>29.12</td>\n",
       "      <td>41.58</td>\n",
       "      <td>31.78</td>\n",
       "      <td>31.13</td>\n",
       "      <td>29.41</td>\n",
       "      <td>35.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision</th>\n",
       "      <td>33.33</td>\n",
       "      <td>16.67</td>\n",
       "      <td>20.13</td>\n",
       "      <td>15.46</td>\n",
       "      <td>31.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.85</td>\n",
       "      <td>26.42</td>\n",
       "      <td>26.88</td>\n",
       "      <td>18.33</td>\n",
       "      <td>14.47</td>\n",
       "      <td>30.43</td>\n",
       "      <td>20.75</td>\n",
       "      <td>22.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision_rag</th>\n",
       "      <td>60.61</td>\n",
       "      <td>26.04</td>\n",
       "      <td>37.11</td>\n",
       "      <td>30.93</td>\n",
       "      <td>51.16</td>\n",
       "      <td>3.33</td>\n",
       "      <td>32.58</td>\n",
       "      <td>41.87</td>\n",
       "      <td>39.25</td>\n",
       "      <td>31.67</td>\n",
       "      <td>25.00</td>\n",
       "      <td>49.57</td>\n",
       "      <td>29.25</td>\n",
       "      <td>36.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl</th>\n",
       "      <td>73.75</td>\n",
       "      <td>64.58</td>\n",
       "      <td>63.52</td>\n",
       "      <td>76.29</td>\n",
       "      <td>84.81</td>\n",
       "      <td>74.07</td>\n",
       "      <td>74.53</td>\n",
       "      <td>74.39</td>\n",
       "      <td>80.11</td>\n",
       "      <td>78.06</td>\n",
       "      <td>69.61</td>\n",
       "      <td>58.41</td>\n",
       "      <td>74.73</td>\n",
       "      <td>53.85</td>\n",
       "      <td>80.00</td>\n",
       "      <td>82.24</td>\n",
       "      <td>66.98</td>\n",
       "      <td>71.28</td>\n",
       "      <td>73.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl_rag</th>\n",
       "      <td>82.50</td>\n",
       "      <td>76.04</td>\n",
       "      <td>71.70</td>\n",
       "      <td>83.51</td>\n",
       "      <td>85.44</td>\n",
       "      <td>82.10</td>\n",
       "      <td>80.90</td>\n",
       "      <td>81.30</td>\n",
       "      <td>87.10</td>\n",
       "      <td>83.87</td>\n",
       "      <td>79.41</td>\n",
       "      <td>68.14</td>\n",
       "      <td>79.12</td>\n",
       "      <td>66.48</td>\n",
       "      <td>83.68</td>\n",
       "      <td>86.29</td>\n",
       "      <td>76.42</td>\n",
       "      <td>80.97</td>\n",
       "      <td>80.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Alternative Investments  Corporate Finance  \\\n",
       "gpt_o1                                  67.50              67.71   \n",
       "gpt_o1_rag                              77.50              83.33   \n",
       "Gemini_Pro1                             76.25              65.62   \n",
       "Gemini_Pro1_rag                         81.25              78.12   \n",
       "deepseek                                73.75              69.79   \n",
       "deepseek_rag                            80.00              80.21   \n",
       "llama                                   36.25              37.50   \n",
       "llama_rag                               44.87              46.88   \n",
       "gpt_4o                                  76.25              75.79   \n",
       "gpt_4o_rag                              88.75              87.37   \n",
       "gemini_1.5_pro                          81.25              83.33   \n",
       "gemini_1.5_pro_rag                      90.00              90.62   \n",
       "qwen                                    71.25              56.25   \n",
       "qwen_rag                                85.00              71.88   \n",
       "Llava                                   26.25              25.00   \n",
       "Llava_rag                               38.75              35.42   \n",
       "Llama_3.2_vision                        33.33              16.67   \n",
       "Llama_3.2_vision_rag                    60.61              26.04   \n",
       "qwen2_vl                                73.75              64.58   \n",
       "qwen2_vl_rag                            82.50              76.04   \n",
       "\n",
       "                      Credit Risk Measurement and Management  Derivatives  \\\n",
       "gpt_o1                                                 71.07        65.98   \n",
       "gpt_o1_rag                                             81.76        79.38   \n",
       "Gemini_Pro1                                            49.37        80.41   \n",
       "Gemini_Pro1_rag                                        62.42        83.51   \n",
       "deepseek                                               58.49        73.20   \n",
       "deepseek_rag                                           65.41        82.47   \n",
       "llama                                                  49.69        26.80   \n",
       "llama_rag                                              59.87        41.24   \n",
       "gpt_4o                                                 72.90        85.57   \n",
       "gpt_4o_rag                                             81.29        89.69   \n",
       "gemini_1.5_pro                                         66.67        80.41   \n",
       "gemini_1.5_pro_rag                                     74.84        89.69   \n",
       "qwen                                                   53.46        62.89   \n",
       "qwen_rag                                               67.30        74.23   \n",
       "Llava                                                  22.64        35.05   \n",
       "Llava_rag                                              28.30        45.36   \n",
       "Llama_3.2_vision                                       20.13        15.46   \n",
       "Llama_3.2_vision_rag                                   37.11        30.93   \n",
       "qwen2_vl                                               63.52        76.29   \n",
       "qwen2_vl_rag                                           71.70        83.51   \n",
       "\n",
       "                      Economics  Equity Investments  \\\n",
       "gpt_o1                    74.68               63.58   \n",
       "gpt_o1_rag                85.99               77.78   \n",
       "Gemini_Pro1               81.65               76.54   \n",
       "Gemini_Pro1_rag           84.81               84.57   \n",
       "deepseek                  80.38               76.54   \n",
       "deepseek_rag              88.61               82.10   \n",
       "llama                     43.04               38.89   \n",
       "llama_rag                 58.86               49.38   \n",
       "gpt_4o                    89.03               86.42   \n",
       "gpt_4o_rag                90.32               88.89   \n",
       "gemini_1.5_pro            86.71               83.33   \n",
       "gemini_1.5_pro_rag        92.41               92.59   \n",
       "qwen                      77.85               75.31   \n",
       "qwen_rag                  84.81               83.33   \n",
       "Llava                     35.44               34.57   \n",
       "Llava_rag                 39.87               43.21   \n",
       "Llama_3.2_vision          31.01                0.00   \n",
       "Llama_3.2_vision_rag      51.16                3.33   \n",
       "qwen2_vl                  84.81               74.07   \n",
       "qwen2_vl_rag              85.44               82.10   \n",
       "\n",
       "                      Financial Markets and Products  \\\n",
       "gpt_o1                                         74.16   \n",
       "gpt_o1_rag                                     87.64   \n",
       "Gemini_Pro1                                    76.32   \n",
       "Gemini_Pro1_rag                                84.91   \n",
       "deepseek                                       72.66   \n",
       "deepseek_rag                                   81.65   \n",
       "llama                                          25.09   \n",
       "llama_rag                                      40.47   \n",
       "gpt_4o                                         81.58   \n",
       "gpt_4o_rag                                     87.97   \n",
       "gemini_1.5_pro                                 75.66   \n",
       "gemini_1.5_pro_rag                             84.27   \n",
       "qwen                                           64.04   \n",
       "qwen_rag                                       74.53   \n",
       "Llava                                          26.59   \n",
       "Llava_rag                                      31.84   \n",
       "Llama_3.2_vision                               16.85   \n",
       "Llama_3.2_vision_rag                           32.58   \n",
       "qwen2_vl                                       74.53   \n",
       "qwen2_vl_rag                                   80.90   \n",
       "\n",
       "                      Financial Reporting and Analysis  Fixed Income  \\\n",
       "gpt_o1                                           67.07         78.49   \n",
       "gpt_o1_rag                                       77.33         88.71   \n",
       "Gemini_Pro1                                      71.43         59.12   \n",
       "Gemini_Pro1_rag                                  78.78         69.36   \n",
       "deepseek                                         73.17         77.96   \n",
       "deepseek_rag                                     80.08         83.33   \n",
       "llama                                            35.37         67.20   \n",
       "llama_rag                                        46.34         77.96   \n",
       "gpt_4o                                           85.31         88.52   \n",
       "gpt_4o_rag                                       89.80         91.80   \n",
       "gemini_1.5_pro                                   79.67         82.26   \n",
       "gemini_1.5_pro_rag                               86.59         89.78   \n",
       "qwen                                             67.48         69.89   \n",
       "qwen_rag                                         78.05         79.03   \n",
       "Llava                                            36.59         38.71   \n",
       "Llava_rag                                        43.90         47.31   \n",
       "Llama_3.2_vision                                 26.42         26.88   \n",
       "Llama_3.2_vision_rag                             41.87         39.25   \n",
       "qwen2_vl                                         74.39         80.11   \n",
       "qwen2_vl_rag                                     81.30         87.10   \n",
       "\n",
       "                      Foundation of Risk Management  \\\n",
       "gpt_o1                                        63.23   \n",
       "gpt_o1_rag                                    75.97   \n",
       "Gemini_Pro1                                   66.45   \n",
       "Gemini_Pro1_rag                               72.90   \n",
       "deepseek                                      69.68   \n",
       "deepseek_rag                                  74.19   \n",
       "llama                                         32.90   \n",
       "llama_rag                                     48.37   \n",
       "gpt_4o                                        84.40   \n",
       "gpt_4o_rag                                    88.73   \n",
       "gemini_1.5_pro                                84.52   \n",
       "gemini_1.5_pro_rag                            88.39   \n",
       "qwen                                          68.39   \n",
       "qwen_rag                                      78.06   \n",
       "Llava                                         24.52   \n",
       "Llava_rag                                     30.32   \n",
       "Llama_3.2_vision                              18.33   \n",
       "Llama_3.2_vision_rag                          31.67   \n",
       "qwen2_vl                                      78.06   \n",
       "qwen2_vl_rag                                  83.87   \n",
       "\n",
       "                      Liquidity and Treasury Risk Measurement  \\\n",
       "gpt_o1                                                  61.76   \n",
       "gpt_o1_rag                                              76.24   \n",
       "Gemini_Pro1                                             67.65   \n",
       "Gemini_Pro1_rag                                         80.39   \n",
       "deepseek                                                53.92   \n",
       "deepseek_rag                                            70.59   \n",
       "llama                                                   24.51   \n",
       "llama_rag                                               42.00   \n",
       "gpt_4o                                                  74.49   \n",
       "gpt_4o_rag                                              82.65   \n",
       "gemini_1.5_pro                                          69.61   \n",
       "gemini_1.5_pro_rag                                      81.37   \n",
       "qwen                                                    52.94   \n",
       "qwen_rag                                                67.65   \n",
       "Llava                                                   17.65   \n",
       "Llava_rag                                               25.49   \n",
       "Llama_3.2_vision                                        14.47   \n",
       "Llama_3.2_vision_rag                                    25.00   \n",
       "qwen2_vl                                                69.61   \n",
       "qwen2_vl_rag                                            79.41   \n",
       "\n",
       "                      Market Risk Measurement and Management  \\\n",
       "gpt_o1                                                 63.72   \n",
       "gpt_o1_rag                                             74.34   \n",
       "Gemini_Pro1                                            64.60   \n",
       "Gemini_Pro1_rag                                        72.57   \n",
       "deepseek                                               55.75   \n",
       "deepseek_rag                                           63.72   \n",
       "llama                                                  19.47   \n",
       "llama_rag                                              35.40   \n",
       "gpt_4o                                                 63.39   \n",
       "gpt_4o_rag                                             71.43   \n",
       "gemini_1.5_pro                                         65.49   \n",
       "gemini_1.5_pro_rag                                     76.99   \n",
       "qwen                                                   40.71   \n",
       "qwen_rag                                               61.95   \n",
       "Llava                                                  23.01   \n",
       "Llava_rag                                              30.97   \n",
       "Llama_3.2_vision                                       30.43   \n",
       "Llama_3.2_vision_rag                                   49.57   \n",
       "qwen2_vl                                               58.41   \n",
       "qwen2_vl_rag                                           68.14   \n",
       "\n",
       "                      Operational Risk  Portfolio Management  \\\n",
       "gpt_o1                           57.14                 51.10   \n",
       "gpt_o1_rag                       70.33                 65.38   \n",
       "Gemini_Pro1                      71.43                 58.01   \n",
       "Gemini_Pro1_rag                  81.32                 70.98   \n",
       "deepseek                         64.84                 54.40   \n",
       "deepseek_rag                     71.43                 63.89   \n",
       "llama                            37.36                 26.92   \n",
       "llama_rag                        50.55                 35.71   \n",
       "gpt_4o                           69.66                 63.22   \n",
       "gpt_4o_rag                       73.03                 74.14   \n",
       "gemini_1.5_pro                   71.43                 59.34   \n",
       "gemini_1.5_pro_rag               76.92                 76.37   \n",
       "qwen                             67.03                 50.00   \n",
       "qwen_rag                         78.02                 60.44   \n",
       "Llava                            35.16                 17.03   \n",
       "Llava_rag                        39.56                 29.12   \n",
       "Llama_3.2_vision                 20.75                 22.06   \n",
       "Llama_3.2_vision_rag             29.25                 36.88   \n",
       "qwen2_vl                         74.73                 53.85   \n",
       "qwen2_vl_rag                     79.12                 66.48   \n",
       "\n",
       "                      Quantitative Methods  \\\n",
       "gpt_o1                               65.79   \n",
       "gpt_o1_rag                           80.42   \n",
       "Gemini_Pro1                          76.84   \n",
       "Gemini_Pro1_rag                      83.68   \n",
       "deepseek                             78.95   \n",
       "deepseek_rag                         85.79   \n",
       "llama                                31.58   \n",
       "llama_rag                            47.37   \n",
       "gpt_4o                               85.26   \n",
       "gpt_4o_rag                           87.89   \n",
       "gemini_1.5_pro                       82.63   \n",
       "gemini_1.5_pro_rag                   87.37   \n",
       "qwen                                 72.11   \n",
       "qwen_rag                             78.95   \n",
       "Llava                                34.74   \n",
       "Llava_rag                            41.58   \n",
       "Llama_3.2_vision                       NaN   \n",
       "Llama_3.2_vision_rag                   NaN   \n",
       "qwen2_vl                             80.00   \n",
       "qwen2_vl_rag                         83.68   \n",
       "\n",
       "                      Risk Management and Investment Management  \\\n",
       "gpt_o1                                                    77.26   \n",
       "gpt_o1_rag                                                86.92   \n",
       "Gemini_Pro1                                               81.62   \n",
       "Gemini_Pro1_rag                                           85.67   \n",
       "deepseek                                                  82.87   \n",
       "deepseek_rag                                              88.16   \n",
       "llama                                                     40.19   \n",
       "llama_rag                                                 52.34   \n",
       "gpt_4o                                                    85.05   \n",
       "gpt_4o_rag                                                87.54   \n",
       "gemini_1.5_pro                                            89.10   \n",
       "gemini_1.5_pro_rag                                        92.83   \n",
       "qwen                                                      82.35   \n",
       "qwen_rag                                                  88.85   \n",
       "Llava                                                     28.97   \n",
       "Llava_rag                                                 31.78   \n",
       "Llama_3.2_vision                                            NaN   \n",
       "Llama_3.2_vision_rag                                        NaN   \n",
       "qwen2_vl                                                  82.24   \n",
       "qwen2_vl_rag                                              86.29   \n",
       "\n",
       "                      Valuation and Risk Models  Others  Overall  \n",
       "gpt_o1                                    55.66   67.13    67.67  \n",
       "gpt_o1_rag                                70.75   84.08    80.41  \n",
       "Gemini_Pro1                               60.38   68.51    70.39  \n",
       "Gemini_Pro1_rag                           70.75   79.58    78.68  \n",
       "deepseek                                  63.21   69.20    70.90  \n",
       "deepseek_rag                              73.58   79.58    78.75  \n",
       "llama                                     26.42   26.99    35.20  \n",
       "llama_rag                                 40.00   38.49    47.95  \n",
       "gpt_4o                                    70.87   75.00    79.69  \n",
       "gpt_4o_rag                                80.58   81.60    85.28  \n",
       "gemini_1.5_pro                            73.58   70.59    77.53  \n",
       "gemini_1.5_pro_rag                        80.19   86.16    86.00  \n",
       "qwen                                      65.09   63.32    66.02  \n",
       "qwen_rag                                  75.47   76.47    76.68  \n",
       "Llava                                     25.47   24.57    28.73  \n",
       "Llava_rag                                 31.13   29.41    35.47  \n",
       "Llama_3.2_vision                            NaN     NaN      NaN  \n",
       "Llama_3.2_vision_rag                        NaN     NaN      NaN  \n",
       "qwen2_vl                                  66.98   71.28    73.27  \n",
       "qwen2_vl_rag                              76.42   80.97    80.40  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_Acc=[]\n",
    "for model in models:\n",
    "    topic_Acc.append(evaluate_topics(file_path(model)))\n",
    "topic_result=pd.DataFrame(topic_Acc,index=models, columns=column)\n",
    "topic_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_result.to_csv('/Volumes/Jennie/Reasoning/FinReasoning/evaluation/Topic_Accuracy1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alternative Investments</th>\n",
       "      <th>Corporate Finance</th>\n",
       "      <th>Credit Risk Measurement and Management</th>\n",
       "      <th>Derivatives</th>\n",
       "      <th>Economics</th>\n",
       "      <th>Equity Investments</th>\n",
       "      <th>Financial Markets and Products</th>\n",
       "      <th>Financial Reporting and Analysis</th>\n",
       "      <th>Fixed Income</th>\n",
       "      <th>Foundation of Risk Management</th>\n",
       "      <th>Liquidity and Treasury Risk Measurement</th>\n",
       "      <th>Market Risk Measurement and Management</th>\n",
       "      <th>Operational Risk</th>\n",
       "      <th>Portfolio Management</th>\n",
       "      <th>Quantitative Methods</th>\n",
       "      <th>Risk Management and Investment Management</th>\n",
       "      <th>Valuation and Risk Models</th>\n",
       "      <th>Others</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt_o1</th>\n",
       "      <td>57.45</td>\n",
       "      <td>54.05</td>\n",
       "      <td>35.71</td>\n",
       "      <td>60.61</td>\n",
       "      <td>54.17</td>\n",
       "      <td>36.59</td>\n",
       "      <td>36.36</td>\n",
       "      <td>38.27</td>\n",
       "      <td>60.38</td>\n",
       "      <td>17.65</td>\n",
       "      <td>16.67</td>\n",
       "      <td>38.46</td>\n",
       "      <td>100.00</td>\n",
       "      <td>45.22</td>\n",
       "      <td>53.57</td>\n",
       "      <td>40.48</td>\n",
       "      <td>31.58</td>\n",
       "      <td>30.43</td>\n",
       "      <td>45.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_o1_rag</th>\n",
       "      <td>70.21</td>\n",
       "      <td>67.57</td>\n",
       "      <td>50.00</td>\n",
       "      <td>69.70</td>\n",
       "      <td>62.50</td>\n",
       "      <td>53.66</td>\n",
       "      <td>60.00</td>\n",
       "      <td>48.78</td>\n",
       "      <td>75.47</td>\n",
       "      <td>35.29</td>\n",
       "      <td>33.33</td>\n",
       "      <td>46.15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>61.74</td>\n",
       "      <td>67.86</td>\n",
       "      <td>54.76</td>\n",
       "      <td>57.89</td>\n",
       "      <td>47.83</td>\n",
       "      <td>60.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1</th>\n",
       "      <td>68.09</td>\n",
       "      <td>51.35</td>\n",
       "      <td>28.57</td>\n",
       "      <td>66.67</td>\n",
       "      <td>58.33</td>\n",
       "      <td>31.71</td>\n",
       "      <td>36.36</td>\n",
       "      <td>35.80</td>\n",
       "      <td>50.98</td>\n",
       "      <td>5.88</td>\n",
       "      <td>50.00</td>\n",
       "      <td>38.46</td>\n",
       "      <td>100.00</td>\n",
       "      <td>53.04</td>\n",
       "      <td>60.71</td>\n",
       "      <td>35.71</td>\n",
       "      <td>36.84</td>\n",
       "      <td>21.74</td>\n",
       "      <td>47.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1_rag</th>\n",
       "      <td>76.60</td>\n",
       "      <td>64.86</td>\n",
       "      <td>50.00</td>\n",
       "      <td>66.67</td>\n",
       "      <td>66.67</td>\n",
       "      <td>56.10</td>\n",
       "      <td>72.73</td>\n",
       "      <td>47.50</td>\n",
       "      <td>66.04</td>\n",
       "      <td>23.53</td>\n",
       "      <td>75.00</td>\n",
       "      <td>61.54</td>\n",
       "      <td>100.00</td>\n",
       "      <td>69.53</td>\n",
       "      <td>75.00</td>\n",
       "      <td>42.86</td>\n",
       "      <td>52.63</td>\n",
       "      <td>39.13</td>\n",
       "      <td>61.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek</th>\n",
       "      <td>61.70</td>\n",
       "      <td>70.27</td>\n",
       "      <td>64.29</td>\n",
       "      <td>63.64</td>\n",
       "      <td>75.00</td>\n",
       "      <td>63.41</td>\n",
       "      <td>54.55</td>\n",
       "      <td>51.85</td>\n",
       "      <td>60.38</td>\n",
       "      <td>58.82</td>\n",
       "      <td>58.33</td>\n",
       "      <td>30.77</td>\n",
       "      <td>100.00</td>\n",
       "      <td>49.57</td>\n",
       "      <td>71.43</td>\n",
       "      <td>73.81</td>\n",
       "      <td>52.63</td>\n",
       "      <td>65.22</td>\n",
       "      <td>60.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_rag</th>\n",
       "      <td>72.34</td>\n",
       "      <td>83.78</td>\n",
       "      <td>78.57</td>\n",
       "      <td>84.85</td>\n",
       "      <td>91.67</td>\n",
       "      <td>70.73</td>\n",
       "      <td>63.64</td>\n",
       "      <td>64.20</td>\n",
       "      <td>64.15</td>\n",
       "      <td>70.59</td>\n",
       "      <td>75.00</td>\n",
       "      <td>46.15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>60.18</td>\n",
       "      <td>77.38</td>\n",
       "      <td>83.33</td>\n",
       "      <td>63.16</td>\n",
       "      <td>73.91</td>\n",
       "      <td>71.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen</th>\n",
       "      <td>63.83</td>\n",
       "      <td>56.76</td>\n",
       "      <td>35.71</td>\n",
       "      <td>54.55</td>\n",
       "      <td>50.00</td>\n",
       "      <td>63.41</td>\n",
       "      <td>27.27</td>\n",
       "      <td>50.62</td>\n",
       "      <td>66.04</td>\n",
       "      <td>29.41</td>\n",
       "      <td>41.67</td>\n",
       "      <td>30.77</td>\n",
       "      <td>100.00</td>\n",
       "      <td>48.70</td>\n",
       "      <td>61.90</td>\n",
       "      <td>71.43</td>\n",
       "      <td>52.63</td>\n",
       "      <td>56.52</td>\n",
       "      <td>55.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_rag</th>\n",
       "      <td>82.98</td>\n",
       "      <td>78.38</td>\n",
       "      <td>35.71</td>\n",
       "      <td>69.70</td>\n",
       "      <td>70.83</td>\n",
       "      <td>70.73</td>\n",
       "      <td>45.45</td>\n",
       "      <td>62.96</td>\n",
       "      <td>75.47</td>\n",
       "      <td>52.94</td>\n",
       "      <td>66.67</td>\n",
       "      <td>46.15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>60.87</td>\n",
       "      <td>67.86</td>\n",
       "      <td>80.95</td>\n",
       "      <td>68.42</td>\n",
       "      <td>78.26</td>\n",
       "      <td>68.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama</th>\n",
       "      <td>29.79</td>\n",
       "      <td>27.03</td>\n",
       "      <td>21.43</td>\n",
       "      <td>18.18</td>\n",
       "      <td>37.50</td>\n",
       "      <td>26.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.93</td>\n",
       "      <td>49.06</td>\n",
       "      <td>17.65</td>\n",
       "      <td>8.33</td>\n",
       "      <td>7.69</td>\n",
       "      <td>50.00</td>\n",
       "      <td>28.70</td>\n",
       "      <td>28.57</td>\n",
       "      <td>35.71</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.74</td>\n",
       "      <td>27.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_rag</th>\n",
       "      <td>31.91</td>\n",
       "      <td>40.54</td>\n",
       "      <td>21.43</td>\n",
       "      <td>24.24</td>\n",
       "      <td>50.00</td>\n",
       "      <td>29.27</td>\n",
       "      <td>30.00</td>\n",
       "      <td>35.80</td>\n",
       "      <td>66.04</td>\n",
       "      <td>17.65</td>\n",
       "      <td>9.09</td>\n",
       "      <td>7.69</td>\n",
       "      <td>100.00</td>\n",
       "      <td>36.52</td>\n",
       "      <td>39.29</td>\n",
       "      <td>42.86</td>\n",
       "      <td>5.26</td>\n",
       "      <td>34.78</td>\n",
       "      <td>36.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_sonnet</th>\n",
       "      <td>65.96</td>\n",
       "      <td>59.46</td>\n",
       "      <td>50.00</td>\n",
       "      <td>69.70</td>\n",
       "      <td>54.17</td>\n",
       "      <td>39.02</td>\n",
       "      <td>36.36</td>\n",
       "      <td>46.91</td>\n",
       "      <td>60.38</td>\n",
       "      <td>37.50</td>\n",
       "      <td>25.00</td>\n",
       "      <td>30.77</td>\n",
       "      <td>100.00</td>\n",
       "      <td>55.65</td>\n",
       "      <td>65.48</td>\n",
       "      <td>47.62</td>\n",
       "      <td>47.37</td>\n",
       "      <td>56.52</td>\n",
       "      <td>54.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_sonnet_rag</th>\n",
       "      <td>72.34</td>\n",
       "      <td>70.27</td>\n",
       "      <td>50.00</td>\n",
       "      <td>81.82</td>\n",
       "      <td>66.67</td>\n",
       "      <td>51.22</td>\n",
       "      <td>36.36</td>\n",
       "      <td>60.49</td>\n",
       "      <td>71.70</td>\n",
       "      <td>62.50</td>\n",
       "      <td>50.00</td>\n",
       "      <td>46.15</td>\n",
       "      <td>100.00</td>\n",
       "      <td>67.83</td>\n",
       "      <td>76.19</td>\n",
       "      <td>61.90</td>\n",
       "      <td>47.37</td>\n",
       "      <td>56.52</td>\n",
       "      <td>65.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o</th>\n",
       "      <td>70.21</td>\n",
       "      <td>72.22</td>\n",
       "      <td>71.43</td>\n",
       "      <td>75.76</td>\n",
       "      <td>73.91</td>\n",
       "      <td>70.73</td>\n",
       "      <td>63.64</td>\n",
       "      <td>73.75</td>\n",
       "      <td>78.00</td>\n",
       "      <td>82.35</td>\n",
       "      <td>66.67</td>\n",
       "      <td>38.46</td>\n",
       "      <td>100.00</td>\n",
       "      <td>62.96</td>\n",
       "      <td>77.38</td>\n",
       "      <td>76.19</td>\n",
       "      <td>77.78</td>\n",
       "      <td>78.26</td>\n",
       "      <td>72.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o_rag</th>\n",
       "      <td>85.11</td>\n",
       "      <td>88.89</td>\n",
       "      <td>85.71</td>\n",
       "      <td>84.85</td>\n",
       "      <td>78.26</td>\n",
       "      <td>75.61</td>\n",
       "      <td>72.73</td>\n",
       "      <td>82.50</td>\n",
       "      <td>78.00</td>\n",
       "      <td>94.12</td>\n",
       "      <td>75.00</td>\n",
       "      <td>76.92</td>\n",
       "      <td>100.00</td>\n",
       "      <td>75.93</td>\n",
       "      <td>80.95</td>\n",
       "      <td>83.33</td>\n",
       "      <td>83.33</td>\n",
       "      <td>91.30</td>\n",
       "      <td>81.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro</th>\n",
       "      <td>76.60</td>\n",
       "      <td>89.19</td>\n",
       "      <td>50.00</td>\n",
       "      <td>78.79</td>\n",
       "      <td>70.83</td>\n",
       "      <td>63.41</td>\n",
       "      <td>54.55</td>\n",
       "      <td>64.20</td>\n",
       "      <td>75.47</td>\n",
       "      <td>82.35</td>\n",
       "      <td>66.67</td>\n",
       "      <td>53.85</td>\n",
       "      <td>100.00</td>\n",
       "      <td>58.26</td>\n",
       "      <td>75.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>94.74</td>\n",
       "      <td>69.57</td>\n",
       "      <td>70.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro_rag</th>\n",
       "      <td>87.23</td>\n",
       "      <td>91.89</td>\n",
       "      <td>71.43</td>\n",
       "      <td>87.88</td>\n",
       "      <td>87.50</td>\n",
       "      <td>85.37</td>\n",
       "      <td>63.64</td>\n",
       "      <td>71.60</td>\n",
       "      <td>81.13</td>\n",
       "      <td>94.12</td>\n",
       "      <td>75.00</td>\n",
       "      <td>84.62</td>\n",
       "      <td>100.00</td>\n",
       "      <td>73.91</td>\n",
       "      <td>83.33</td>\n",
       "      <td>90.48</td>\n",
       "      <td>100.00</td>\n",
       "      <td>86.96</td>\n",
       "      <td>82.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_3.5</th>\n",
       "      <td>82.98</td>\n",
       "      <td>91.89</td>\n",
       "      <td>57.14</td>\n",
       "      <td>84.85</td>\n",
       "      <td>70.83</td>\n",
       "      <td>65.85</td>\n",
       "      <td>72.73</td>\n",
       "      <td>67.90</td>\n",
       "      <td>81.13</td>\n",
       "      <td>76.47</td>\n",
       "      <td>66.67</td>\n",
       "      <td>53.85</td>\n",
       "      <td>100.00</td>\n",
       "      <td>74.78</td>\n",
       "      <td>77.38</td>\n",
       "      <td>88.10</td>\n",
       "      <td>78.95</td>\n",
       "      <td>73.91</td>\n",
       "      <td>76.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_3.5_rag</th>\n",
       "      <td>87.23</td>\n",
       "      <td>91.89</td>\n",
       "      <td>64.29</td>\n",
       "      <td>87.88</td>\n",
       "      <td>79.17</td>\n",
       "      <td>73.17</td>\n",
       "      <td>72.73</td>\n",
       "      <td>75.31</td>\n",
       "      <td>88.68</td>\n",
       "      <td>82.35</td>\n",
       "      <td>66.67</td>\n",
       "      <td>61.54</td>\n",
       "      <td>100.00</td>\n",
       "      <td>77.39</td>\n",
       "      <td>82.14</td>\n",
       "      <td>90.48</td>\n",
       "      <td>89.47</td>\n",
       "      <td>78.26</td>\n",
       "      <td>80.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl</th>\n",
       "      <td>63.83</td>\n",
       "      <td>59.46</td>\n",
       "      <td>35.71</td>\n",
       "      <td>60.61</td>\n",
       "      <td>58.33</td>\n",
       "      <td>36.59</td>\n",
       "      <td>18.18</td>\n",
       "      <td>43.21</td>\n",
       "      <td>69.81</td>\n",
       "      <td>29.41</td>\n",
       "      <td>33.33</td>\n",
       "      <td>23.08</td>\n",
       "      <td>50.00</td>\n",
       "      <td>54.78</td>\n",
       "      <td>65.48</td>\n",
       "      <td>47.62</td>\n",
       "      <td>52.63</td>\n",
       "      <td>60.87</td>\n",
       "      <td>53.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl_rag</th>\n",
       "      <td>76.60</td>\n",
       "      <td>72.97</td>\n",
       "      <td>57.14</td>\n",
       "      <td>75.76</td>\n",
       "      <td>58.33</td>\n",
       "      <td>51.22</td>\n",
       "      <td>36.36</td>\n",
       "      <td>59.26</td>\n",
       "      <td>79.25</td>\n",
       "      <td>47.06</td>\n",
       "      <td>66.67</td>\n",
       "      <td>38.46</td>\n",
       "      <td>50.00</td>\n",
       "      <td>65.22</td>\n",
       "      <td>73.81</td>\n",
       "      <td>57.14</td>\n",
       "      <td>63.16</td>\n",
       "      <td>65.22</td>\n",
       "      <td>65.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava</th>\n",
       "      <td>23.40</td>\n",
       "      <td>16.22</td>\n",
       "      <td>7.14</td>\n",
       "      <td>15.15</td>\n",
       "      <td>25.00</td>\n",
       "      <td>12.20</td>\n",
       "      <td>9.09</td>\n",
       "      <td>19.75</td>\n",
       "      <td>20.75</td>\n",
       "      <td>11.76</td>\n",
       "      <td>8.33</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.87</td>\n",
       "      <td>21.43</td>\n",
       "      <td>4.76</td>\n",
       "      <td>5.26</td>\n",
       "      <td>4.35</td>\n",
       "      <td>16.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava_rag</th>\n",
       "      <td>31.91</td>\n",
       "      <td>24.32</td>\n",
       "      <td>7.14</td>\n",
       "      <td>39.39</td>\n",
       "      <td>29.17</td>\n",
       "      <td>29.27</td>\n",
       "      <td>18.18</td>\n",
       "      <td>30.86</td>\n",
       "      <td>39.62</td>\n",
       "      <td>11.76</td>\n",
       "      <td>33.33</td>\n",
       "      <td>30.77</td>\n",
       "      <td>50.00</td>\n",
       "      <td>27.83</td>\n",
       "      <td>26.19</td>\n",
       "      <td>14.29</td>\n",
       "      <td>31.58</td>\n",
       "      <td>30.43</td>\n",
       "      <td>28.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.47</td>\n",
       "      <td>5.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision_rag</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.09</td>\n",
       "      <td>16.67</td>\n",
       "      <td>3.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.88</td>\n",
       "      <td>9.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.26</td>\n",
       "      <td>9.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Alternative Investments  Corporate Finance  \\\n",
       "gpt_o1                                  57.45              54.05   \n",
       "gpt_o1_rag                              70.21              67.57   \n",
       "Gemini_Pro1                             68.09              51.35   \n",
       "Gemini_Pro1_rag                         76.60              64.86   \n",
       "deepseek                                61.70              70.27   \n",
       "deepseek_rag                            72.34              83.78   \n",
       "qwen                                    63.83              56.76   \n",
       "qwen_rag                                82.98              78.38   \n",
       "llama                                   29.79              27.03   \n",
       "llama_rag                               31.91              40.54   \n",
       "claude_sonnet                           65.96              59.46   \n",
       "claude_sonnet_rag                       72.34              70.27   \n",
       "gpt_4o                                  70.21              72.22   \n",
       "gpt_4o_rag                              85.11              88.89   \n",
       "gemini_1.5_pro                          76.60              89.19   \n",
       "gemini_1.5_pro_rag                      87.23              91.89   \n",
       "claude_3.5                              82.98              91.89   \n",
       "claude_3.5_rag                          87.23              91.89   \n",
       "qwen2_vl                                63.83              59.46   \n",
       "qwen2_vl_rag                            76.60              72.97   \n",
       "Llava                                   23.40              16.22   \n",
       "Llava_rag                               31.91              24.32   \n",
       "Llama_3.2_vision                         0.00               0.00   \n",
       "Llama_3.2_vision_rag                     0.00               0.00   \n",
       "\n",
       "                      Credit Risk Measurement and Management  Derivatives  \\\n",
       "gpt_o1                                                 35.71        60.61   \n",
       "gpt_o1_rag                                             50.00        69.70   \n",
       "Gemini_Pro1                                            28.57        66.67   \n",
       "Gemini_Pro1_rag                                        50.00        66.67   \n",
       "deepseek                                               64.29        63.64   \n",
       "deepseek_rag                                           78.57        84.85   \n",
       "qwen                                                   35.71        54.55   \n",
       "qwen_rag                                               35.71        69.70   \n",
       "llama                                                  21.43        18.18   \n",
       "llama_rag                                              21.43        24.24   \n",
       "claude_sonnet                                          50.00        69.70   \n",
       "claude_sonnet_rag                                      50.00        81.82   \n",
       "gpt_4o                                                 71.43        75.76   \n",
       "gpt_4o_rag                                             85.71        84.85   \n",
       "gemini_1.5_pro                                         50.00        78.79   \n",
       "gemini_1.5_pro_rag                                     71.43        87.88   \n",
       "claude_3.5                                             57.14        84.85   \n",
       "claude_3.5_rag                                         64.29        87.88   \n",
       "qwen2_vl                                               35.71        60.61   \n",
       "qwen2_vl_rag                                           57.14        75.76   \n",
       "Llava                                                   7.14        15.15   \n",
       "Llava_rag                                               7.14        39.39   \n",
       "Llama_3.2_vision                                        3.03         0.00   \n",
       "Llama_3.2_vision_rag                                    9.09        16.67   \n",
       "\n",
       "                      Economics  Equity Investments  \\\n",
       "gpt_o1                    54.17               36.59   \n",
       "gpt_o1_rag                62.50               53.66   \n",
       "Gemini_Pro1               58.33               31.71   \n",
       "Gemini_Pro1_rag           66.67               56.10   \n",
       "deepseek                  75.00               63.41   \n",
       "deepseek_rag              91.67               70.73   \n",
       "qwen                      50.00               63.41   \n",
       "qwen_rag                  70.83               70.73   \n",
       "llama                     37.50               26.83   \n",
       "llama_rag                 50.00               29.27   \n",
       "claude_sonnet             54.17               39.02   \n",
       "claude_sonnet_rag         66.67               51.22   \n",
       "gpt_4o                    73.91               70.73   \n",
       "gpt_4o_rag                78.26               75.61   \n",
       "gemini_1.5_pro            70.83               63.41   \n",
       "gemini_1.5_pro_rag        87.50               85.37   \n",
       "claude_3.5                70.83               65.85   \n",
       "claude_3.5_rag            79.17               73.17   \n",
       "qwen2_vl                  58.33               36.59   \n",
       "qwen2_vl_rag              58.33               51.22   \n",
       "Llava                     25.00               12.20   \n",
       "Llava_rag                 29.17               29.27   \n",
       "Llama_3.2_vision           0.00                0.00   \n",
       "Llama_3.2_vision_rag       3.85                0.00   \n",
       "\n",
       "                      Financial Markets and Products  \\\n",
       "gpt_o1                                         36.36   \n",
       "gpt_o1_rag                                     60.00   \n",
       "Gemini_Pro1                                    36.36   \n",
       "Gemini_Pro1_rag                                72.73   \n",
       "deepseek                                       54.55   \n",
       "deepseek_rag                                   63.64   \n",
       "qwen                                           27.27   \n",
       "qwen_rag                                       45.45   \n",
       "llama                                           0.00   \n",
       "llama_rag                                      30.00   \n",
       "claude_sonnet                                  36.36   \n",
       "claude_sonnet_rag                              36.36   \n",
       "gpt_4o                                         63.64   \n",
       "gpt_4o_rag                                     72.73   \n",
       "gemini_1.5_pro                                 54.55   \n",
       "gemini_1.5_pro_rag                             63.64   \n",
       "claude_3.5                                     72.73   \n",
       "claude_3.5_rag                                 72.73   \n",
       "qwen2_vl                                       18.18   \n",
       "qwen2_vl_rag                                   36.36   \n",
       "Llava                                           9.09   \n",
       "Llava_rag                                      18.18   \n",
       "Llama_3.2_vision                                2.47   \n",
       "Llama_3.2_vision_rag                            9.88   \n",
       "\n",
       "                      Financial Reporting and Analysis  Fixed Income  \\\n",
       "gpt_o1                                           38.27         60.38   \n",
       "gpt_o1_rag                                       48.78         75.47   \n",
       "Gemini_Pro1                                      35.80         50.98   \n",
       "Gemini_Pro1_rag                                  47.50         66.04   \n",
       "deepseek                                         51.85         60.38   \n",
       "deepseek_rag                                     64.20         64.15   \n",
       "qwen                                             50.62         66.04   \n",
       "qwen_rag                                         62.96         75.47   \n",
       "llama                                            25.93         49.06   \n",
       "llama_rag                                        35.80         66.04   \n",
       "claude_sonnet                                    46.91         60.38   \n",
       "claude_sonnet_rag                                60.49         71.70   \n",
       "gpt_4o                                           73.75         78.00   \n",
       "gpt_4o_rag                                       82.50         78.00   \n",
       "gemini_1.5_pro                                   64.20         75.47   \n",
       "gemini_1.5_pro_rag                               71.60         81.13   \n",
       "claude_3.5                                       67.90         81.13   \n",
       "claude_3.5_rag                                   75.31         88.68   \n",
       "qwen2_vl                                         43.21         69.81   \n",
       "qwen2_vl_rag                                     59.26         79.25   \n",
       "Llava                                            19.75         20.75   \n",
       "Llava_rag                                        30.86         39.62   \n",
       "Llama_3.2_vision                                  5.66          0.00   \n",
       "Llama_3.2_vision_rag                              9.43          0.00   \n",
       "\n",
       "                      Foundation of Risk Management  \\\n",
       "gpt_o1                                        17.65   \n",
       "gpt_o1_rag                                    35.29   \n",
       "Gemini_Pro1                                    5.88   \n",
       "Gemini_Pro1_rag                               23.53   \n",
       "deepseek                                      58.82   \n",
       "deepseek_rag                                  70.59   \n",
       "qwen                                          29.41   \n",
       "qwen_rag                                      52.94   \n",
       "llama                                         17.65   \n",
       "llama_rag                                     17.65   \n",
       "claude_sonnet                                 37.50   \n",
       "claude_sonnet_rag                             62.50   \n",
       "gpt_4o                                        82.35   \n",
       "gpt_4o_rag                                    94.12   \n",
       "gemini_1.5_pro                                82.35   \n",
       "gemini_1.5_pro_rag                            94.12   \n",
       "claude_3.5                                    76.47   \n",
       "claude_3.5_rag                                82.35   \n",
       "qwen2_vl                                      29.41   \n",
       "qwen2_vl_rag                                  47.06   \n",
       "Llava                                         11.76   \n",
       "Llava_rag                                     11.76   \n",
       "Llama_3.2_vision                               8.82   \n",
       "Llama_3.2_vision_rag                          20.59   \n",
       "\n",
       "                      Liquidity and Treasury Risk Measurement  \\\n",
       "gpt_o1                                                  16.67   \n",
       "gpt_o1_rag                                              33.33   \n",
       "Gemini_Pro1                                             50.00   \n",
       "Gemini_Pro1_rag                                         75.00   \n",
       "deepseek                                                58.33   \n",
       "deepseek_rag                                            75.00   \n",
       "qwen                                                    41.67   \n",
       "qwen_rag                                                66.67   \n",
       "llama                                                    8.33   \n",
       "llama_rag                                                9.09   \n",
       "claude_sonnet                                           25.00   \n",
       "claude_sonnet_rag                                       50.00   \n",
       "gpt_4o                                                  66.67   \n",
       "gpt_4o_rag                                              75.00   \n",
       "gemini_1.5_pro                                          66.67   \n",
       "gemini_1.5_pro_rag                                      75.00   \n",
       "claude_3.5                                              66.67   \n",
       "claude_3.5_rag                                          66.67   \n",
       "qwen2_vl                                                33.33   \n",
       "qwen2_vl_rag                                            66.67   \n",
       "Llava                                                    8.33   \n",
       "Llava_rag                                               33.33   \n",
       "Llama_3.2_vision                                         0.00   \n",
       "Llama_3.2_vision_rag                                     0.00   \n",
       "\n",
       "                      Market Risk Measurement and Management  \\\n",
       "gpt_o1                                                 38.46   \n",
       "gpt_o1_rag                                             46.15   \n",
       "Gemini_Pro1                                            38.46   \n",
       "Gemini_Pro1_rag                                        61.54   \n",
       "deepseek                                               30.77   \n",
       "deepseek_rag                                           46.15   \n",
       "qwen                                                   30.77   \n",
       "qwen_rag                                               46.15   \n",
       "llama                                                   7.69   \n",
       "llama_rag                                               7.69   \n",
       "claude_sonnet                                          30.77   \n",
       "claude_sonnet_rag                                      46.15   \n",
       "gpt_4o                                                 38.46   \n",
       "gpt_4o_rag                                             76.92   \n",
       "gemini_1.5_pro                                         53.85   \n",
       "gemini_1.5_pro_rag                                     84.62   \n",
       "claude_3.5                                             53.85   \n",
       "claude_3.5_rag                                         61.54   \n",
       "qwen2_vl                                               23.08   \n",
       "qwen2_vl_rag                                           38.46   \n",
       "Llava                                                   7.69   \n",
       "Llava_rag                                              30.77   \n",
       "Llama_3.2_vision                                        0.00   \n",
       "Llama_3.2_vision_rag                                    5.26   \n",
       "\n",
       "                      Operational Risk  Portfolio Management  \\\n",
       "gpt_o1                          100.00                 45.22   \n",
       "gpt_o1_rag                      100.00                 61.74   \n",
       "Gemini_Pro1                     100.00                 53.04   \n",
       "Gemini_Pro1_rag                 100.00                 69.53   \n",
       "deepseek                        100.00                 49.57   \n",
       "deepseek_rag                    100.00                 60.18   \n",
       "qwen                            100.00                 48.70   \n",
       "qwen_rag                        100.00                 60.87   \n",
       "llama                            50.00                 28.70   \n",
       "llama_rag                       100.00                 36.52   \n",
       "claude_sonnet                   100.00                 55.65   \n",
       "claude_sonnet_rag               100.00                 67.83   \n",
       "gpt_4o                          100.00                 62.96   \n",
       "gpt_4o_rag                      100.00                 75.93   \n",
       "gemini_1.5_pro                  100.00                 58.26   \n",
       "gemini_1.5_pro_rag              100.00                 73.91   \n",
       "claude_3.5                      100.00                 74.78   \n",
       "claude_3.5_rag                  100.00                 77.39   \n",
       "qwen2_vl                         50.00                 54.78   \n",
       "qwen2_vl_rag                     50.00                 65.22   \n",
       "Llava                             0.00                 20.87   \n",
       "Llava_rag                        50.00                 27.83   \n",
       "Llama_3.2_vision                  3.31                   NaN   \n",
       "Llama_3.2_vision_rag              9.09                   NaN   \n",
       "\n",
       "                      Quantitative Methods  \\\n",
       "gpt_o1                               53.57   \n",
       "gpt_o1_rag                           67.86   \n",
       "Gemini_Pro1                          60.71   \n",
       "Gemini_Pro1_rag                      75.00   \n",
       "deepseek                             71.43   \n",
       "deepseek_rag                         77.38   \n",
       "qwen                                 61.90   \n",
       "qwen_rag                             67.86   \n",
       "llama                                28.57   \n",
       "llama_rag                            39.29   \n",
       "claude_sonnet                        65.48   \n",
       "claude_sonnet_rag                    76.19   \n",
       "gpt_4o                               77.38   \n",
       "gpt_4o_rag                           80.95   \n",
       "gemini_1.5_pro                       75.00   \n",
       "gemini_1.5_pro_rag                   83.33   \n",
       "claude_3.5                           77.38   \n",
       "claude_3.5_rag                       82.14   \n",
       "qwen2_vl                             65.48   \n",
       "qwen2_vl_rag                         73.81   \n",
       "Llava                                21.43   \n",
       "Llava_rag                            26.19   \n",
       "Llama_3.2_vision                       NaN   \n",
       "Llama_3.2_vision_rag                   NaN   \n",
       "\n",
       "                      Risk Management and Investment Management  \\\n",
       "gpt_o1                                                    40.48   \n",
       "gpt_o1_rag                                                54.76   \n",
       "Gemini_Pro1                                               35.71   \n",
       "Gemini_Pro1_rag                                           42.86   \n",
       "deepseek                                                  73.81   \n",
       "deepseek_rag                                              83.33   \n",
       "qwen                                                      71.43   \n",
       "qwen_rag                                                  80.95   \n",
       "llama                                                     35.71   \n",
       "llama_rag                                                 42.86   \n",
       "claude_sonnet                                             47.62   \n",
       "claude_sonnet_rag                                         61.90   \n",
       "gpt_4o                                                    76.19   \n",
       "gpt_4o_rag                                                83.33   \n",
       "gemini_1.5_pro                                            83.33   \n",
       "gemini_1.5_pro_rag                                        90.48   \n",
       "claude_3.5                                                88.10   \n",
       "claude_3.5_rag                                            90.48   \n",
       "qwen2_vl                                                  47.62   \n",
       "qwen2_vl_rag                                              57.14   \n",
       "Llava                                                      4.76   \n",
       "Llava_rag                                                 14.29   \n",
       "Llama_3.2_vision                                            NaN   \n",
       "Llama_3.2_vision_rag                                        NaN   \n",
       "\n",
       "                      Valuation and Risk Models  Others  Overall  \n",
       "gpt_o1                                    31.58   30.43    45.81  \n",
       "gpt_o1_rag                                57.89   47.83    60.18  \n",
       "Gemini_Pro1                               36.84   21.74    47.45  \n",
       "Gemini_Pro1_rag                           52.63   39.13    61.91  \n",
       "deepseek                                  52.63   65.22    60.63  \n",
       "deepseek_rag                              63.16   73.91    71.17  \n",
       "qwen                                      52.63   56.52    55.09  \n",
       "qwen_rag                                  68.42   78.26    68.11  \n",
       "llama                                      0.00   21.74    27.40  \n",
       "llama_rag                                  5.26   34.78    36.19  \n",
       "claude_sonnet                             47.37   56.52    54.27  \n",
       "claude_sonnet_rag                         47.37   56.52    65.37  \n",
       "gpt_4o                                    77.78   78.26    72.02  \n",
       "gpt_4o_rag                                83.33   91.30    81.35  \n",
       "gemini_1.5_pro                            94.74   69.57    70.81  \n",
       "gemini_1.5_pro_rag                       100.00   86.96    82.04  \n",
       "claude_3.5                                78.95   73.91    76.20  \n",
       "claude_3.5_rag                            89.47   78.26    80.99  \n",
       "qwen2_vl                                  52.63   60.87    53.14  \n",
       "qwen2_vl_rag                              63.16   65.22    65.12  \n",
       "Llava                                      5.26    4.35    16.77  \n",
       "Llava_rag                                 31.58   30.43    28.29  \n",
       "Llama_3.2_vision                            NaN     NaN      NaN  \n",
       "Llama_3.2_vision_rag                        NaN     NaN      NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_Acc=[]\n",
    "for model in models:\n",
    "    topic_Acc.append(evaluate_topics_test(file_path(model)))\n",
    "topic_result_test=pd.DataFrame(topic_Acc,index=models, columns=column)\n",
    "topic_result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_result_test.to_csv('/Users/sden118/Desktop/FinReasoning/evaluation/Topic_Accuracy_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>error type</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>calculation</th>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image</th>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>option</th>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>2315</td>\n",
       "      <td>2315</td>\n",
       "      <td>2315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  error type  Model\n",
       "Error                               \n",
       "calculation   276         276    276\n",
       "image         102         102    102\n",
       "option        147         147    147\n",
       "other         338         338    338\n",
       "question     2315        2315   2315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/sden118/Desktop/FinReasoning/evaluation/errorType.csv')\n",
    "data.groupby(['Error']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
